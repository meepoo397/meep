{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e346519b",
   "metadata": {},
   "source": [
    "# 請直接覆蓋原本的DatasetGenerator的_data_preprocess\n",
    "# 我只有改_data_preprocess\n",
    "# 或整個class覆蓋也可以"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11bbd9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetGenerator:\n",
    "    \"\"\"\n",
    "    Load pascalVOC 2007 dataset and creates an input pipeline.\n",
    "    - Reshapes images into 448 x 448\n",
    "    - converts [0 1] to [-1 1]\n",
    "    - shuffles the input\n",
    "    - builds batches\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.image_names = []\n",
    "        self.record_list = []\n",
    "        self.object_num_list = []\n",
    "        # filling the record_list\n",
    "        input_file = open(DATA_PATH, 'r')\n",
    "\n",
    "        for line in input_file:\n",
    "            line = line.strip()\n",
    "            ss = line.split(' ')\n",
    "            self.image_names.append(ss[0])\n",
    "\n",
    "            self.record_list.append([float(num) for num in ss[1:]])\n",
    "\n",
    "            self.object_num_list.append(min(len(self.record_list[-1])//5,\n",
    "                                            MAX_OBJECTS_PER_IMAGE))\n",
    "            if len(self.record_list[-1]) < MAX_OBJECTS_PER_IMAGE*5:\n",
    "                # if there are objects less than MAX_OBJECTS_PER_IMAGE, pad the list\n",
    "                self.record_list[-1] = self.record_list[-1] +\\\n",
    "                [0., 0., 0., 0., 0.]*\\\n",
    "                (MAX_OBJECTS_PER_IMAGE-len(self.record_list[-1])//5)\n",
    "\n",
    "            elif len(self.record_list[-1]) > MAX_OBJECTS_PER_IMAGE*5:\n",
    "               # if there are objects more than MAX_OBJECTS_PER_IMAGE, crop the list\n",
    "                self.record_list[-1] = self.record_list[-1][:MAX_OBJECTS_PER_IMAGE*5]\n",
    "\n",
    "    def _data_preprocess(self, image_name,raw_labels, object_num):\n",
    "        image_file = tf.io.read_file(IMAGE_DIR+image_name)\n",
    "        image = tf.io.decode_jpeg(image_file, channels=3)\n",
    "        \n",
    "        # Apply data augmentation and record transformations\n",
    "        \n",
    "        # flip_horizontal: boolean value of whether flip horizontal\n",
    "        # dx: translation precentage of image on x axis ex. 0.05=>move right 0.05*IMAGE_SIZE pixels\n",
    "        # dy: similar on y axis\n",
    "        # crop_scale: central crop scale in 0.8~1.0\n",
    "        \n",
    "        flip_horizontal = tf.cast(tf.random.uniform([], 0, 2, dtype=tf.int32), tf.bool)\n",
    "        dx = np.random.uniform(-0.1, 0.1)\n",
    "        dy = np.random.uniform(-0.1, 0.1)\n",
    "        crop_scale = tf.random.uniform([], 0.8, 1.0)\n",
    "        \n",
    "        # 1. Flip horizontal\n",
    "        w_flip = tf.cast(tf.shape(image)[1], tf.float32)\n",
    "        image = tf.cond(flip_horizontal, \n",
    "                       lambda: tf.image.flip_left_right(image),\n",
    "                       lambda: image)\n",
    "        # 2. Shift\n",
    "        def translate_image(image, tx, ty):\n",
    "            \"\"\"平移影像\n",
    "            Args:\n",
    "                image: 輸入影像 (3D Tensor)。\n",
    "                tx: 水平平移量（正數向右，負數向左）。\n",
    "                ty: 垂直平移量（正數向下，負數向上）。\n",
    "            Returns:\n",
    "                平移後的影像。\n",
    "            \"\"\"\n",
    "            # 建立平移矩陣\n",
    "            transform = [1, 0, -tx, 0, 1, -ty, 0, 0]  # 平移矩陣\n",
    "            image = tf.expand_dims(image, axis=0)    # 增加 batch 維度\n",
    "            output = tf.raw_ops.ImageProjectiveTransformV3(\n",
    "                images=image,\n",
    "                transforms=[transform],\n",
    "                #output_shape=image.shape[1:3],\n",
    "                output_shape=(IMAGE_SIZE,IMAGE_SIZE),\n",
    "                interpolation=\"BILINEAR\",\n",
    "                fill_value=0\n",
    "            )\n",
    "            return tf.squeeze(output, axis=0)  # 去除 batch 維度\n",
    "        image = translate_image(image, dx * IMAGE_SIZE, dy * IMAGE_SIZE)\n",
    "        # 3. Central crop\n",
    "        h = tf.cast(tf.shape(image)[0], tf.float32)\n",
    "        w = tf.cast(tf.shape(image)[1], tf.float32)\n",
    "        image = tf.image.central_crop(image, crop_scale)\n",
    "        # 4. Brightness, Contrast, Gaussian noise\n",
    "        image = tf.image.random_brightness(image, 0.2)\n",
    "        image = tf.image.random_contrast(image, 0.8, 1.2)\n",
    "        image = tf.cast(image, tf.float32)  # Ensure image is float32 for arithmetic operations\n",
    "        image = image + tf.random.normal(shape=tf.shape(image), mean=0.0, stddev=0.1, dtype=tf.float32)\n",
    "        image = tf.clip_by_value(image, 0.0, 255.0)  # Keep the pixel values in the valid range\n",
    "        image = tf.cast(image, tf.uint8)  # Convert back to uint8 if necessary\n",
    "\n",
    "        h_after = tf.shape(image)[0]\n",
    "        w_after = tf.shape(image)[1]\n",
    "        width_ratio  = IMAGE_SIZE * 1.0 / tf.cast(w_after, tf.float32)\n",
    "        height_ratio = IMAGE_SIZE * 1.0 / tf.cast(h_after, tf.float32)\n",
    "        # 5. Resize\n",
    "        image = tf.image.resize(image, size=[IMAGE_SIZE, IMAGE_SIZE])\n",
    "        image = (image/255) * 2 - 1\n",
    "\n",
    "        raw_labels = tf.cast(tf.reshape(raw_labels, [-1, 5]), tf.float32)\n",
    "\n",
    "        # Modify labels to correctly record bounding box\n",
    "        xmin = raw_labels[:, 0]\n",
    "        ymin = raw_labels[:, 1]\n",
    "        xmax = raw_labels[:, 2]\n",
    "        ymax = raw_labels[:, 3]\n",
    "        class_num = raw_labels[:, 4]\n",
    "        # 1. Flip\n",
    "        xmin_new = tf.cond(flip_horizontal, lambda: w_flip - xmin, lambda: xmin)\n",
    "        xmax_new = tf.cond(flip_horizontal, lambda: w_flip - xmax, lambda: xmax)\n",
    "        xmin=tf.where(xmin_new>xmax_new,xmax_new, xmin_new)\n",
    "        xmax=tf.where(xmin_new>xmax_new, xmin_new, xmax_new)\n",
    "\n",
    "        # 2. Shift\n",
    "        xmin = (xmin + dx * IMAGE_SIZE)\n",
    "        xmax = (xmax + dx * IMAGE_SIZE)\n",
    "        ymin = (ymin + dy * IMAGE_SIZE)\n",
    "        ymax = (ymax + dy * IMAGE_SIZE)\n",
    "        \n",
    "        # 3. Central crop\n",
    "        xmin = xmin - w*(1-crop_scale)/2\n",
    "        xmax = xmax - w*(1-crop_scale)/2\n",
    "        ymin = ymin - h*(1-crop_scale)/2\n",
    "        ymax = ymax - h*(1-crop_scale)/2\n",
    "        \n",
    "        # 4. Brightness, Contrast, Gaussian noise\n",
    "        # 5. Resize\n",
    "        xmin = xmin * width_ratio\n",
    "        ymin = ymin * height_ratio\n",
    "        xmax = xmax * width_ratio\n",
    "        ymax = ymax * height_ratio\n",
    "        \n",
    "        # move the values into range (0~IMAGE_SIZE) if possible\n",
    "        xmin=tf.where(xmin>=0,xmin,0.)\n",
    "        xmax=tf.where(xmax<IMAGE_SIZE,xmax,IMAGE_SIZE*1.0)\n",
    "        ymin=tf.where(ymin>=0,ymin,0.)\n",
    "        ymax=tf.where(ymax<IMAGE_SIZE,ymax,IMAGE_SIZE*1.0)\n",
    "        # If any value is out of range, set as a padding box\n",
    "        range_mask=tf.range(raw_labels.shape[0],dtype=tf.int32) < int(object_num)\n",
    "        edge_mask = tf.logical_and(xmin < xmax, ymin < ymax)\n",
    "        valid_mask = tf.logical_and(range_mask,edge_mask)\n",
    "\n",
    "        # Get indices of valid objects\n",
    "        valid_indices = tf.where(valid_mask)[:, 0]\n",
    "\n",
    "        # Update object_num to reflect the number of valid objects\n",
    "        object_num = tf.shape(valid_indices)[0]\n",
    "\n",
    "        # Select valid values for xmin, xmax, ymin, ymax\n",
    "        xmin_valid = tf.gather(xmin, valid_indices)\n",
    "        xmax_valid = tf.gather(xmax, valid_indices)\n",
    "        ymin_valid = tf.gather(ymin, valid_indices)\n",
    "        ymax_valid = tf.gather(ymax, valid_indices)\n",
    "\n",
    "        # Pad the valid tensors to match MAX_OBJECTS_PER_IMAGE\n",
    "        padding_size = MAX_OBJECTS_PER_IMAGE - tf.shape(valid_indices)[0]\n",
    "        xmin_padded = tf.concat([xmin_valid, tf.zeros([padding_size], dtype=tf.float32)], axis=0)\n",
    "        xmax_padded = tf.concat([xmax_valid, tf.zeros([padding_size], dtype=tf.float32)], axis=0)\n",
    "        ymin_padded = tf.concat([ymin_valid, tf.zeros([padding_size], dtype=tf.float32)], axis=0)\n",
    "        ymax_padded = tf.concat([ymax_valid, tf.zeros([padding_size], dtype=tf.float32)], axis=0)\n",
    "\n",
    "        # Compute xcenter, ycenter, box_w, and box_h\n",
    "        xcenter = (xmin_padded + xmax_padded) / 2.0\n",
    "        ycenter = (ymin_padded + ymax_padded) / 2.0\n",
    "        box_w = tf.math.abs(xmax_padded - xmin_padded)\n",
    "        box_h = tf.math.abs(ymax_padded - ymin_padded)\n",
    "\n",
    "        # Create the skip mask\n",
    "        skip_mask = tf.range(MAX_OBJECTS_PER_IMAGE) < object_num\n",
    "\n",
    "        # Create padding box and combine labels\n",
    "        padding_box = tf.zeros([5], dtype=tf.float32)\n",
    "        labels = tf.where(\n",
    "            tf.expand_dims(skip_mask, axis=1),\n",
    "            tf.stack([xcenter, ycenter, box_w, box_h, class_num[:MAX_OBJECTS_PER_IMAGE]], axis=1),\n",
    "            tf.expand_dims(padding_box, axis=0)\n",
    "        )\n",
    "        \n",
    "        return image, labels, tf.cast(object_num, tf.int32)\n",
    "\n",
    "    def generate(self):\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((self.image_names,\n",
    "                                                      np.array(self.record_list),\n",
    "                                                      np.array(self.object_num_list)))\n",
    "        dataset = dataset.shuffle(100000)\n",
    "        dataset = dataset.map(self._data_preprocess,\n",
    "                              num_parallel_calls = tf.data.experimental.AUTOTUNE)\n",
    "        dataset = dataset.batch(BATCH_SIZE)\n",
    "        dataset = dataset.prefetch(buffer_size=200)\n",
    "\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14344ed2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
